{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " change the world. It’s already changing it, in fact. From healthcare to transportation, from finance to education, AI is being used to improve efficiency, accuracy, and productivity across a wide range of industries.\n",
      "\n",
      "But as with any technology, there are also potential risks associated with AI. For example, there are concerns about job displacement, privacy, security, and bias. These issues need to be addressed in order to ensure that AI is developed and deployed in a responsible and ethical manner.\n",
      "\n",
      "One way to address these challenges is through the development of standards and guidelines for AI. These could cover topics such as transparency, accountability, fairness, and privacy. They could also provide guidance on how to ensure that AI systems are designed and trained in ways that minimize bias and discrimination.\n",
      "\n",
      "Another important approach is to invest in education and training programs that help people develop the skills they need to work with AI. This could include courses on data science, machine learning, and other related fields. By providing people with the knowledge and skills they need to work with AI, we can help ensure that the benefits of this technology are widely shared.\n",
      "\n",
      "Finally, it’s important to involve a diverse range of stakeholders in the development and deployment of AI. This includes experts from industry, academia, government, and civil society. By bringing together people with different perspectives and experiences, we can help ensure that AI is developed and deployed in ways that are beneficial for everyone.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "config = {\n",
    "    'gpu_layers': 10000,\n",
    "    #'model_type': \"mistral\",\n",
    "    'max_new_tokens' : 1048,\n",
    "    'temperature' : 0.3,\n",
    "    }\n",
    "\n",
    "llm = CTransformers(\n",
    "    model = 'mistral-7b-instruct-v0.1.Q5_K_M.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n",
    "\n",
    "    config = config\n",
    ")\n",
    "\n",
    "print(llm.invoke('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-15 23:58:45 config.py:193] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 03-15 23:58:45 llm_engine.py:87] Initializing an LLM engine with config: model='Qwen/Qwen1.5-14B-Chat-AWQ', tokenizer='Qwen/Qwen1.5-14B-Chat-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/rps/miniconda3/envs/llm/lib/python3.11/site-packages/cupy/_environment.py:404: UserWarning: \n",
      "nccl library could not be loaded.\n",
      "\n",
      "Reason: ImportError (libnccl.so.2: cannot open shared object file: No such file or directory)\n",
      "\n",
      "You can install the library by:\n",
      "\n",
      "  $ python -m cupyx.tools.install_library --library nccl --cuda 12.x\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 23:58:47 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-15 23:59:05 llm_engine.py:357] # GPU blocks: 629, # CPU blocks: 327\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for VLLM\n__root__\n  The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (10064). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# llm = VLLM(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     model = 'Qwen/Qwen1.5-14B-Chat-AWQ',#'qwen1_5-14b-chat-q5_0.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     trust_remote_code=True,  # mandatory for hf models\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     gpu_memory_utilization=1,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VLLM\n\u001b[0;32m---> 15\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen1.5-14B-Chat-AWQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# mandatory for hf models\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# tensor_parallel_size=... # for distributed inference\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France ?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI is going to\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for VLLM\n__root__\n  The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (10064). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "# llm = VLLM(\n",
    "#     model = 'Qwen/Qwen1.5-14B-Chat-AWQ',#'qwen1_5-14b-chat-q5_0.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n",
    "#     trust_remote_code=True,  # mandatory for hf models\n",
    "#     max_new_tokens=1048,\n",
    "#     max_model_len=8192,\n",
    "#     top_k=3,\n",
    "#     top_p=0.95,\n",
    "#     temperature=0.1,\n",
    "#     gpu_memory_utilization=1,\n",
    "# )\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(model = 'Qwen/Qwen1.5-14B-Chat-AWQ',\n",
    "           trust_remote_code=True,  # mandatory for hf models\n",
    "           max_new_tokens=128,\n",
    "           top_k=10,\n",
    "           top_p=0.95,\n",
    "           temperature=0.8,\n",
    "           # tensor_parallel_size=... # for distributed inference\n",
    ")\n",
    "\n",
    "print(llm(\"What is the capital of France ?\"))\n",
    "print(llm.invoke('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create LLM 'gguf' from 'qwen1_5-14b-chat-q5_0.gguf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CTransformers\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#'qwen1_5-14b-chat-q5_0.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1048\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     }\n\u001b[0;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mCTransformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#model = '/home/rps/projects/Question-Answer-Generation-App-using-Mistral-7B/Qwen/qwen1_5-14b-chat-q5_0.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqwen1_5-14b-chat-q5_0.gguf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI is going to\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_community/llms/ctransformers.py:72\u001b[0m, in \u001b[0;36mCTransformers.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import `ctransformers` package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install ctransformers`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m config \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 72\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/ctransformers/hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[1;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    170\u001b[0m         model_file,\n\u001b[1;32m    171\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    172\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/ctransformers/llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[1;32m    248\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    249\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    250\u001b[0m     config\u001b[38;5;241m.\u001b[39mto_struct(),\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create LLM \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m architecture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctransformers_llm_architecture()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to create LLM 'gguf' from 'qwen1_5-14b-chat-q5_0.gguf'."
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "config = {\n",
    "    'gpu_layers': 10000,\n",
    "#'qwen1_5-14b-chat-q5_0.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n",
    "    'max_new_tokens' : 1048,\n",
    "    'temperature' : 0.1,\n",
    "    }\n",
    "\n",
    "llm = CTransformers(\n",
    "    #model = '/home/rps/projects/Question-Answer-Generation-App-using-Mistral-7B/Qwen/qwen1_5-14b-chat-q5_0.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n",
    "    model = 'qwen1_5-14b-chat-q5_0.gguf',\n",
    "    config = config\n",
    ")\n",
    "\n",
    "print(llm.invoke('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf to /home/rps/.cache/huggingface/hub/tmp8_zdd2z9\n",
      "mistral-7b-instruct-v0.1.Q5_K_M.gguf: 100%|█| 5.13G/5.13G [03:14<00:00, 26.4MB/s\n",
      "./mistral-7b-instruct-v0.1.Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download  \\\n",
    "        TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "        mistral-7b-instruct-v0.1.Q5_K_M.gguf   \\\n",
    "        --local-dir Mistral-7B \\\n",
    "        --local-dir-use-symlinks False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import CTransformers\n",
    "# config = {\n",
    "#     'gpu_layers': 10000,\n",
    "#     #'model_type': \"mistral\",\n",
    "#     'max_new_tokens' : 1048,\n",
    "#     'temperature' : 0.3,\n",
    "#     }\n",
    "\n",
    "# llm = CTransformers(\n",
    "#     model = '/Qwen/'#'mistral-7b-instruct-v0.1.Q5_K_M.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n",
    "\n",
    "#     config = config\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download  \\\n",
    "        NousResearch/Nous-Hermes-2-SOLAR-10.7B \\\n",
    "        --local-dir NousResearch \\\n",
    "        --local-dir-use-symlinks False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/Qwen/Qwen1.5-14B-Chat-GGUF/resolve/main/qwen1_5-14b-chat-q5_0.gguf to /home/rps/.cache/huggingface/hub/tmpnfzuaxq9\n",
      "qwen1_5-14b-chat-q5_0.gguf: 100%|██████████| 9.85G/9.85G [04:22<00:00, 37.5MB/s]\n",
      "./Qwen/qwen1_5-14b-chat-q5_0.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download \\\n",
    "    Qwen/Qwen1.5-14B-Chat-GGUF \\\n",
    "    qwen1_5-14b-chat-q5_0.gguf \\\n",
    "    --local-dir './Qwen' \\\n",
    "    --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'qwen2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# the device to load the model onto\u001b[39;00m\n\u001b[1;32m      5\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Qwen/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device_map=\"auto\",\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# load_in_8bit=True,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#device=device,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#low_cpu_mem_usage= True,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#quantization_config=quantization_config,\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Qwen/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:527\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 527\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1041\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m-> 1041\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:734\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    735\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    736\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'qwen2'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    './Qwen/',\n",
    "    torch_dtype= torch.bfloat16,\n",
    "    # device_map=\"auto\",\n",
    "    # load_in_8bit=True,\n",
    "    #device=device,\n",
    "    #low_cpu_mem_usage= True,\n",
    "    #quantization_config=quantization_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('./Qwen/')\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(llm.invoke('AI is going to'))\n",
    "# prompt = \"Give me a short introduction to large language model.\"\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     model_inputs.input_ids,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download  \\\n",
    "        Qwen/Qwen1.5-14B-Chat \\\n",
    "        --local-dir Qwen \\\n",
    "        --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF/resolve/main/nous-hermes-2-solar-10.7b.Q5_K_M.gguf to /home/rps/.cache/huggingface/hub/tmpf1zyjjyq\n",
      "nous-hermes-2-solar-10.7b.Q5_K_M.gguf: 100%|█| 7.60G/7.60G [03:22<00:00, 37.6MB/\n",
      "NousResearch/gguf/nous-hermes-2-solar-10.7b.Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF \\\n",
    "    nous-hermes-2-solar-10.7b.Q5_K_M.gguf \\\n",
    "    --local-dir ./NousResearch/gguf \\\n",
    "    --local-dir-use-symlinks False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be a game-changer in the world of healthcare.\n",
      "AI is already being used to improve patient outcomes, reduce costs and increase efficiency in hospitals and clinics around the world.\n",
      "In this article, we'll take a look at some of the ways that AI is transforming healthcare.\n",
      "1. Improving Diagnosis Accuracy\n",
      "One of the most significant ways that AI is changing healthcare is by improving diagnosis accuracy.\n",
      "AI algorithms can analyze large amounts of data from medical records, lab results, and imaging studies to identify patterns and make more accurate diagnoses than human doctors.\n",
      "This can help to reduce misdiagnoses and improve patient outcomes.\n",
      "2. Predictive Analytics\n",
      "Another way that AI is transforming healthcare is through predictive analytics.\n",
      "AI algorithms can analyze data from electronic health records, wearable devices, and other sources to identify patients who are at risk for certain conditions or diseases.\n",
      "This allows doctors to intervene early and prevent serious health problems from developing.\n",
      "3. Personalized Medicine\n",
      "AI is also helping to revolutionize personalized medicine by allowing doctors to tailor treatments to individual patients based on their unique genetic makeup, medical history, and lifestyle factors.\n",
      "By analyzing large amounts of data from a patient's electronic health record, AI algorithms can identify the most effective treatment options for that particular patient.\n",
      "4. Virtual Health Assistants\n",
      "AI is also being used to develop virtual health assistants that can help patients manage their chronic conditions at home.\n",
      "These virtual assistants use natural language processing and machine learning algorithms to understand a patient's symptoms, provide personalized advice, and even schedule appointments with doctors.\n",
      "5. Drug Discovery\n",
      "Finally, AI is also transforming the field of drug discovery by allowing researchers to identify new targets for drugs and design more effective treatments.\n",
      "AI algorithms can analyze large amounts of data from scientific literature, clinical trials, and other sources to identify potential drug candidates that might not have been identified through traditional methods.\n",
      "In conclusion, AI is going to be a game-changer in the world of healthcare. It has the potential to improve diagnosis accuracy, predict health risks, personalize treatments, assist patients at home, and discover new drugs. As AI technology continues to advance, we can expect to see even more exciting developments in the field of healthcare.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.llms import GPT4All\n",
    "# local_path = (\n",
    "#     './NousResearch/'  # replace with your desired local file path\n",
    "# )\n",
    "# llm = GPT4All(model=local_path, device='gpu')\n",
    "from langchain_community.llms import CTransformers\n",
    "config = {\n",
    "    'gpu_layers': 10000,\n",
    "    #'model_type': \"mistral\",\n",
    "    'max_new_tokens' : 2048,\n",
    "    'context_length' : 4096,\n",
    "    'temperature' : 0.3,\n",
    "    }\n",
    "#'/NousResearch/gguf/nous-hermes-2-solar-10.7b.Q5_K_M.gguf'\n",
    "llm = CTransformers(\n",
    "    model = '/home/rps/projects/Question-Answer-Generation-App-using-Mistral-7B/NousResearch/gguf/nous-hermes-2-solar-10.7b.Q5_K_M.gguf',#\"mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n",
    "\n",
    "    config = config\n",
    ")\n",
    "print(llm.invoke('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2175bbc0a2f5479ca9d084cf44d62c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/rps/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# llm = HuggingFacePipeline.from_model_id(\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#    './NousResearch/',\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     task=\"text-generation\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     model_kwargs={\"temperature\": 0, \"max_length\": 1000},\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAI is going to\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:246\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    244\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    257\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:541\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    535\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    540\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:714\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m         )\n\u001b[1;32m    701\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    702\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    703\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    713\u001b[0m     ]\n\u001b[0;32m--> 714\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:578\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    577\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    579\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:565\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    557\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 565\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    573\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_community/llms/huggingface_pipeline.py:261\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:205\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:268\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1586\u001b[0m         input_ids,\n\u001b[1;32m   1587\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1599\u001b[0m     )\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:2511\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2508\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2510\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2512\u001b[0m     this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:125\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStoppingCriteriaList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(criteria(input_ids, scores) \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_length\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# nao funciona\n",
    "import torch\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    './NousResearch/',\n",
    "    torch_dtype= torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    # load_in_8bit=True,\n",
    "    #device=device,\n",
    "    low_cpu_mem_usage= True,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('./NousResearch/')\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, \n",
    "                tokenizer=tokenizer, max_new_tokens=1000, \n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#    './NousResearch/',\n",
    "#     task=\"text-generation\",\n",
    "#     device_map='cuda',\n",
    "#     #device=0,  # -1 for CPU\n",
    "#     #batch_size=2,  # adjust as needed based on GPU map and model size.\n",
    "#     model_kwargs={\"temperature\": 0, \"max_length\": 1000},\n",
    "# )\n",
    "print(llm.invoke('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video: 'Ele É Especialista Em IA E Já Fez 150 Milhões No Digital | Alan Nicolas - Segredos Da Escala #026'\n",
      "Video download complete!\n"
     ]
    }
   ],
   "source": [
    "import pytube\n",
    "import os\n",
    "url = 'https://www.youtube.com/watch?v=nksadR9ePfw'\n",
    "\n",
    "DATA_YOUTUBE = '/home/rps/projects/Question-Answer-Generation-App-using-Mistral-7B/youtube_data'\n",
    "\n",
    "def extract_text(json_data):\n",
    "    \"\"\"Extracts the transcribed text from the JSON data.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The JSON data structure.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated transcribed text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "    for event in json_data['events']:\n",
    "        if 'segs' in event:\n",
    "            for segment in event['segs']:\n",
    "                text += segment['utf8'] + \" \"  # Add a space after each word\n",
    "\n",
    "    return text.strip()  # Remove any trailing spaces\n",
    "\n",
    "def download_video_and_transcript(youtube_url, output_path=DATA_YOUTUBE):\n",
    "    \"\"\"Downloads video and transcripts from a YouTube URL to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        youtube_url (str): The URL of the YouTube video.\n",
    "        output_path (str, optional): The path to the directory where the files will be saved. \n",
    "                                     Defaults to \"path/to/mydata\".\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        youtube = pytube.YouTube(youtube_url)\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Download the video\n",
    "        video = youtube.streams.get_highest_resolution()\n",
    "        print(f\"Downloading video: '{youtube.title}'\")\n",
    "        video_filepath = video.download(output_path=output_path)\n",
    "        print(\"Video download complete!\")\n",
    "\n",
    "        # Download transcripts (if available)\n",
    "        try:\n",
    "            captions = youtube.captions['a.pt'] \n",
    "            if captions:\n",
    "                # captions_text = captions.generate_srt_captions()\n",
    "                # print(\"Captions download complete!\")\n",
    "\n",
    "                # # Save captions to a file\n",
    "                # captions_filepath = os.path.join(output_path, f\"{youtube.title}.srt\")\n",
    "                # with open(captions_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                #     f.write(captions_text)\n",
    "                caps = captions.json_captions\n",
    "                json_data = extract_text(caps)\n",
    "                text_file = os.path.join(DATA_YOUTUBE, 'captions_json_f.txt')\n",
    "                with open(text_file, 'w') as f:\n",
    "                        f.write(json_data) \n",
    "            else:\n",
    "                print(\"No captions found for this video.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"  # Replace with your YouTube video URL\n",
    "            \n",
    "download_video_and_transcript(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Any',\n",
       " 'BaseTool',\n",
       " 'StructuredTool',\n",
       " 'Tool',\n",
       " '_DEPRECATED_TOOLS',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__getattr__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_import_ainetwork_app',\n",
       " '_import_ainetwork_owner',\n",
       " '_import_ainetwork_rule',\n",
       " '_import_ainetwork_transfer',\n",
       " '_import_ainetwork_value',\n",
       " '_import_arxiv_tool',\n",
       " '_import_azure_cognitive_services_AzureCogsFormRecognizerTool',\n",
       " '_import_azure_cognitive_services_AzureCogsImageAnalysisTool',\n",
       " '_import_azure_cognitive_services_AzureCogsSpeech2TextTool',\n",
       " '_import_azure_cognitive_services_AzureCogsText2SpeechTool',\n",
       " '_import_azure_cognitive_services_AzureCogsTextAnalyticsHealthTool',\n",
       " '_import_bearly_tool',\n",
       " '_import_bing_search_tool_BingSearchResults',\n",
       " '_import_bing_search_tool_BingSearchRun',\n",
       " '_import_brave_search_tool',\n",
       " '_import_cogniswitch_answer_tool',\n",
       " '_import_cogniswitch_knowledge_status_tool',\n",
       " '_import_cogniswitch_store_file_tool',\n",
       " '_import_cogniswitch_store_url_tool',\n",
       " '_import_connery_tool',\n",
       " '_import_ddg_search_tool_DuckDuckGoSearchResults',\n",
       " '_import_ddg_search_tool_DuckDuckGoSearchRun',\n",
       " '_import_e2b_data_analysis',\n",
       " '_import_edenai_EdenAiExplicitImageTool',\n",
       " '_import_edenai_EdenAiObjectDetectionTool',\n",
       " '_import_edenai_EdenAiParsingIDTool',\n",
       " '_import_edenai_EdenAiParsingInvoiceTool',\n",
       " '_import_edenai_EdenAiSpeechToTextTool',\n",
       " '_import_edenai_EdenAiTextModerationTool',\n",
       " '_import_edenai_EdenAiTextToSpeechTool',\n",
       " '_import_edenai_EdenaiTool',\n",
       " '_import_eleven_labs_text2speech',\n",
       " '_import_file_management_CopyFileTool',\n",
       " '_import_file_management_DeleteFileTool',\n",
       " '_import_file_management_FileSearchTool',\n",
       " '_import_file_management_ListDirectoryTool',\n",
       " '_import_file_management_MoveFileTool',\n",
       " '_import_file_management_ReadFileTool',\n",
       " '_import_file_management_WriteFileTool',\n",
       " '_import_gmail_GmailCreateDraft',\n",
       " '_import_gmail_GmailGetMessage',\n",
       " '_import_gmail_GmailGetThread',\n",
       " '_import_gmail_GmailSearch',\n",
       " '_import_gmail_GmailSendMessage',\n",
       " '_import_google_cloud_texttospeech',\n",
       " '_import_google_places_tool',\n",
       " '_import_google_search_tool_GoogleSearchResults',\n",
       " '_import_google_search_tool_GoogleSearchRun',\n",
       " '_import_google_serper_tool_GoogleSerperResults',\n",
       " '_import_google_serper_tool_GoogleSerperRun',\n",
       " '_import_graphql_tool',\n",
       " '_import_human_tool',\n",
       " '_import_ifttt',\n",
       " '_import_interaction_tool',\n",
       " '_import_jira_tool',\n",
       " '_import_json_tool_JsonGetValueTool',\n",
       " '_import_json_tool_JsonListKeysTool',\n",
       " '_import_merriam_webster_tool',\n",
       " '_import_metaphor_search',\n",
       " '_import_nasa_tool',\n",
       " '_import_office365_create_draft_message',\n",
       " '_import_office365_events_search',\n",
       " '_import_office365_messages_search',\n",
       " '_import_office365_send_event',\n",
       " '_import_office365_send_message',\n",
       " '_import_office365_utils',\n",
       " '_import_openapi_utils_api_models',\n",
       " '_import_openapi_utils_openapi_utils',\n",
       " '_import_openweathermap_tool',\n",
       " '_import_playwright_ClickTool',\n",
       " '_import_playwright_CurrentWebPageTool',\n",
       " '_import_playwright_ExtractHyperlinksTool',\n",
       " '_import_playwright_ExtractTextTool',\n",
       " '_import_playwright_GetElementsTool',\n",
       " '_import_playwright_NavigateBackTool',\n",
       " '_import_playwright_NavigateTool',\n",
       " '_import_plugin',\n",
       " '_import_polygon_tool_PolygonAggregates',\n",
       " '_import_polygon_tool_PolygonFinancials',\n",
       " '_import_polygon_tool_PolygonLastQuote',\n",
       " '_import_polygon_tool_PolygonTickerNews',\n",
       " '_import_powerbi_tool_InfoPowerBITool',\n",
       " '_import_powerbi_tool_ListPowerBITool',\n",
       " '_import_powerbi_tool_QueryPowerBITool',\n",
       " '_import_pubmed_tool',\n",
       " '_import_python_tool_PythonAstREPLTool',\n",
       " '_import_python_tool_PythonREPLTool',\n",
       " '_import_reddit_search_RedditSearchRun',\n",
       " '_import_render',\n",
       " '_import_requests_tool_BaseRequestsTool',\n",
       " '_import_requests_tool_RequestsDeleteTool',\n",
       " '_import_requests_tool_RequestsGetTool',\n",
       " '_import_requests_tool_RequestsPatchTool',\n",
       " '_import_requests_tool_RequestsPostTool',\n",
       " '_import_requests_tool_RequestsPutTool',\n",
       " '_import_scenexplain_tool',\n",
       " '_import_searchapi_tool_SearchAPIResults',\n",
       " '_import_searchapi_tool_SearchAPIRun',\n",
       " '_import_searx_search_tool_SearxSearchResults',\n",
       " '_import_searx_search_tool_SearxSearchRun',\n",
       " '_import_shell_tool',\n",
       " '_import_slack_get_channel',\n",
       " '_import_slack_get_message',\n",
       " '_import_slack_schedule_message',\n",
       " '_import_slack_send_message',\n",
       " '_import_sleep_tool',\n",
       " '_import_spark_sql_tool_BaseSparkSQLTool',\n",
       " '_import_spark_sql_tool_InfoSparkSQLTool',\n",
       " '_import_spark_sql_tool_ListSparkSQLTool',\n",
       " '_import_spark_sql_tool_QueryCheckerTool',\n",
       " '_import_spark_sql_tool_QuerySparkSQLTool',\n",
       " '_import_sql_database_tool_BaseSQLDatabaseTool',\n",
       " '_import_sql_database_tool_InfoSQLDatabaseTool',\n",
       " '_import_sql_database_tool_ListSQLDatabaseTool',\n",
       " '_import_sql_database_tool_QuerySQLCheckerTool',\n",
       " '_import_sql_database_tool_QuerySQLDataBaseTool',\n",
       " '_import_stackexchange_tool',\n",
       " '_import_steam_webapi_tool',\n",
       " '_import_steamship_image_generation',\n",
       " '_import_vectorstore_tool_VectorStoreQATool',\n",
       " '_import_vectorstore_tool_VectorStoreQAWithSourcesTool',\n",
       " '_import_wikipedia_tool',\n",
       " '_import_wolfram_alpha_tool',\n",
       " '_import_yahoo_finance_news',\n",
       " '_import_you_tool',\n",
       " '_import_youtube_search',\n",
       " '_import_zapier_tool_ZapierNLAListActions',\n",
       " '_import_zapier_tool_ZapierNLARunAction',\n",
       " 'ainetwork',\n",
       " 'arxiv',\n",
       " 'azure_cognitive_services',\n",
       " 'bearly',\n",
       " 'bing_search',\n",
       " 'brave_search',\n",
       " 'cogniswitch',\n",
       " 'connery',\n",
       " 'convert_to_openai',\n",
       " 'dataforseo_api_search',\n",
       " 'ddg_search',\n",
       " 'e2b_data_analysis',\n",
       " 'edenai',\n",
       " 'eleven_labs',\n",
       " 'file_management',\n",
       " 'gmail',\n",
       " 'golden_query',\n",
       " 'google_cloud',\n",
       " 'google_finance',\n",
       " 'google_jobs',\n",
       " 'google_lens',\n",
       " 'google_places',\n",
       " 'google_scholar',\n",
       " 'google_search',\n",
       " 'google_serper',\n",
       " 'google_trends',\n",
       " 'graphql',\n",
       " 'human',\n",
       " 'ifttt',\n",
       " 'interaction',\n",
       " 'jira',\n",
       " 'json',\n",
       " 'memorize',\n",
       " 'merriam_webster',\n",
       " 'metaphor_search',\n",
       " 'nasa',\n",
       " 'office365',\n",
       " 'openapi',\n",
       " 'openweathermap',\n",
       " 'playwright',\n",
       " 'plugin',\n",
       " 'polygon',\n",
       " 'powerbi',\n",
       " 'pubmed',\n",
       " 'reddit_search',\n",
       " 'requests',\n",
       " 'scenexplain',\n",
       " 'searchapi',\n",
       " 'searx_search',\n",
       " 'shell',\n",
       " 'sleep',\n",
       " 'spark_sql',\n",
       " 'sql_database',\n",
       " 'stackexchange',\n",
       " 'steam',\n",
       " 'steamship_image_generation',\n",
       " 'tool',\n",
       " 'vectorstore',\n",
       " 'wikipedia',\n",
       " 'wolfram_alpha',\n",
       " 'yahoo_finance_news',\n",
       " 'you',\n",
       " 'youtube',\n",
       " 'zapier']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_community.tools\n",
    "\n",
    "\n",
    "dir(langchain_community.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23148.14814814815"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1250000/54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
